{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Assignment 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each exercise, the first cell creates the function and the second cell executes the query and prints the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import mean, min, max, col, avg, monotonically_increasing_id\n",
    "import pyspark.sql.functions as F\n",
    "from operator import add\n",
    "import time\n",
    "\n",
    "file = \"pagecounts-20160101-000000\"\n",
    "sqlContext = SQLContext(sc)\n",
    "rdd = sc.textFile(file)\n",
    "\n",
    "def f1(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF()\n",
    "    \n",
    "    # appearance\n",
    "    df= df_normal.limit(15).toPandas()\n",
    "    for index, row in df.iterrows():\n",
    "        print(row[0], row[1], row[2], row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa %CE%92%CE%84_%CE%95%CF%80%CE%B9%CF%83%CF%84%CE%BF%CE%BB%CE%AE_%CE%99%CF%89%CE%AC%CE%BD%CE%BD%CE%B7/el/%CE%92 1 4854\n",
      "aa %CE%98%CE%B5%CF%8C%CE%B4%CF%89%CF%81%CE%BF%CF%82_%CE%91%CE%84_%CE%9B%CE%AC%CF%83%CE%BA%CE%B1%CF%81%CE%B7%CF%82/el/%CE%98%CE%B5%CF%8C%CE%B4%CF%89%CF%81%CE%BF%CF%82_%CE%91%27_%CE%9B%CE%AC%CF%83%CE%BA%CE%B1%CF%81%CE%B7%CF%82 1 4917\n",
      "aa %CE%9C%CF%89%CE%AC%CE%BC%CE%B5%CE%B8_%CE%95%CE%84/el/%CE%9C%CE%B5%CF%87%CE%BC%CE%AD%CF%84_%CE%95 1 4832\n",
      "aa %CE%A0%CE%B9%CE%B5%CF%81_%CE%9B%27_%CE%91%CE%BD%CF%86%CE%AC%CE%BD/el/%CE%A0%CE%B9%CE%B5%CF%81_%CE%9B 1 4828\n",
      "aa %CE%A3%CE%A4%CE%84_%CE%A3%CF%84%CE%B1%CF%85%CF%81%CE%BF%CF%86%CE%BF%CF%81%CE%AF%CE%B1/el/%CE%A3%CE%A4 1 4819\n",
      "aa %D0%A1%D0%BE%D0%BB%D0%B8_484_%D0%BF.%D0%BC 1 4750\n",
      "aa 271_a.C 1 4675\n",
      "aa Battaglia_di_Qade%C5%A1/it/Battaglia_dell%27Oronte 1 4765\n",
      "aa Category:User_th 1 4770\n",
      "aa Chiron_Elias_Krase 1 4694\n",
      "aa County_Laois/en/Queen%27s_County,_Ireland 1 4752\n",
      "aa Dassault_rafaele 2 9372\n",
      "aa Dyskusja_wikiprojektu:Formu%C5%82a_1/%22/pl/Polacy_w_Formule_1%22 1 4824\n",
      "aa E.Desv 1 4662\n",
      "aa Enclos-apier/fr/Enclos-Apiers_en_C%C3%B4te_d%27Azur 1 4772\n"
     ]
    }
   ],
   "source": [
    "f1(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    print(df_normal.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5046226\n"
     ]
    }
   ],
   "source": [
    "f2(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # convert to double\n",
    "    df_normal = df_normal.withColumn('size', col('size').cast('double'))\n",
    "    df_normal = df_normal.withColumn('hits', col('hits').cast('double'))\n",
    "    \n",
    "    # get stats\n",
    "    x = df_normal.select([mean('size'), min('size'), max('size')]).collect()\n",
    "    result = \"Mean = \" + str(x[0][0]) + \"; Min = \" + str(x[0][1])+ \"; Max = \" + str(x[0][2])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 101423.92964801814; Min = 0.0; Max = 141180155987.0\n"
     ]
    }
   ],
   "source": [
    "print(f3(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # convert to double from string\n",
    "    df_normal = df_normal.withColumn('size', col('size').cast('double'))\n",
    "    df_normal = df_normal.withColumn('hits', col('hits').cast('double'))\n",
    "    \n",
    "    #get max\n",
    "    max_value = df_normal.select(max('size')).collect()[0][0]\n",
    "    l=[max_value]\n",
    "    df = df_normal.where(df_normal.size.isin(l))\n",
    "    \n",
    "    df= df.toPandas()\n",
    "    for index, row in df.iterrows():\n",
    "        print(row[0], row[1], row[2], row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.mw en 5466346.0 141180155987.0\n"
     ]
    }
   ],
   "source": [
    "f4(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one record with maximum page size value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f5(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # convert to double from string\n",
    "    df_normal = df_normal.withColumn('size', col('size').cast('double'))\n",
    "    df_normal = df_normal.withColumn('hits', col('hits').cast('double'))\n",
    "    \n",
    "    # get max\n",
    "    max_value = df_normal.select(max('size')).collect()[0][0]\n",
    "    l=[max_value]\n",
    "    df = df_normal.where(df_normal.size.isin(l))\n",
    "    \n",
    "    # pick most hits if several records\n",
    "    max_hits = df.select(max('hits')).collect()[0][0]\n",
    "    l=[max_hits]\n",
    "    df = df_normal.where(df.hits.isin(l))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.mw en 5466346.0 141180155987.0\n"
     ]
    }
   ],
   "source": [
    "df = f5(rdd).toPandas()\n",
    "for index, row in df.iterrows():\n",
    "    print(row[0], row[1], row[2], row[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f6(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # create title length column\n",
    "    new = df_normal.withColumn(\"length\", F.length('title'))\n",
    "    \n",
    "    # get longest page title\n",
    "    max_value = new.select(max('length')).collect()[0][0]\n",
    "    l=[max_value]\n",
    "    df = new.where(new.length.isin(l))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of query is commented out because query is very long. Please remove \"#\" to execute query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = f6(rdd).toPandas()\n",
    "#for index, row in df.iterrows():\n",
    "    #print(row[1])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f7(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # convert to double from string\n",
    "    df_normal = df_normal.withColumn('size', col('size').cast('double'))\n",
    "    df_normal = df_normal.withColumn('hits', col('hits').cast('double'))\n",
    "    \n",
    "    # get mean\n",
    "    mean_value = df_normal.select(mean('size')).collect()[0][0]\n",
    "    \n",
    "    # filter\n",
    "    df = df_normal.filter(df_normal.size > mean_value)\n",
    "\n",
    "    df = df.rdd\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(code='aa', title='Main_Page', hits=5.0, size=266946.0),\n",
       " Row(code='ab', title='%D0%90%D0%B2%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D0%B0:%D0%90%D1%84%D0%BE%D1%80%D1%83%D0%BC', hits=2.0, size=289426.0),\n",
       " Row(code='ab', title='%D0%90%D2%B3%D3%99%D1%8B%D0%BD%D2%AD%D2%9B%D0%B0%D1%80%D1%80%D0%B0%D1%82%D3%99_%D0%B0%D0%B3%D0%B5%D1%80%D0%B1%D2%9B%D3%99%D0%B0_%D1%80%D1%8B%D1%85%D1%8C%D3%A1%D1%8B%D0%BD%D2%B5%D0%B0', hits=1.0, size=273773.0),\n",
       " Row(code='ab', title='%D0%91%D1%80%D0%B8%D1%82%D0%B0%D0%BD%D0%B8%D0%B0_%D0%94%D1%83_%D0%B0%D0%BC%D0%BE%D0%BD%D0%B0%D1%80%D1%85%D1%86%D3%99%D0%B0', hits=3.0, size=711708.0),\n",
       " Row(code='ab', title='%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80_%D0%9F%D1%83%D1%82%D0%B8%D0%BD', hits=3.0, size=147731.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f7(rdd).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f8(rdd):\n",
    "    # Compute the total number of pageviews for each project\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # convert to double from string\n",
    "    df_normal = df_normal.withColumn('size', col('size').cast('double'))\n",
    "    df_normal = df_normal.withColumn('hits', col('hits').cast('double'))\n",
    "    \n",
    "    # group by project code\n",
    "    sum = df_normal.groupBy(\"code\").sum(\"hits\").toPandas()\n",
    "\n",
    "    for index, row in sum.iterrows():\n",
    "        print(\"Total number of page views for \\\"\" + str(row[0]) + \"\\\" project = \" + str(int(row[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of query is commented out because query is very long. Please remove \"#\" to execute query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f8(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f9(rdd):\n",
    "    # Report the 10 most popular pageviews of all projects , sorted by the total number of hits.\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # convert to double from string\n",
    "    df_normal = df_normal.withColumn('size', col('size').cast('double'))\n",
    "    df_normal = df_normal.withColumn('hits', col('hits').cast('double'))\n",
    "    \n",
    "    query = df_normal.orderBy(['hits'], ascending=False)\n",
    "    \n",
    "    # clean visualization\n",
    "    df= query.limit(10).toPandas()\n",
    "    for index, row in df.iterrows():\n",
    "        print(row[0], row[1], row[2], row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.mw en 5466346.0 141180155987.0\n",
      "es.mw es 695531.0 12261337515.0\n",
      "ja.mw ja 611443.0 15021588551.0\n",
      "de.mw de 572119.0 9523069696.0\n",
      "fr.mw fr 536978.0 11752030020.0\n",
      "ru.mw ru 466742.0 11847816616.0\n",
      "it.mw it 400297.0 8176042087.0\n",
      "en Main_Page 257915.0 4289970372.0\n",
      "pt.mw pt 196160.0 4029404403.0\n",
      "pl.mw pl 176059.0 2782453516.0\n"
     ]
    }
   ],
   "source": [
    "f9(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f10(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # Determine the number of page titles that start with the article ”The”.\n",
    "    # How many of those page titles are not part of the English project?\n",
    "    query = df_normal.withColumn('The', df_normal['title'].substr(0, 3)=='The')\n",
    "    query = query.filter(query.The == True)\n",
    "    print(\"Number of titles that start with ”The” = \"+str(query.count()))\n",
    "    query = query.filter(query.code!='en')\n",
    "    print(\"Number of titles that start with ”The” and are not in \\\"en\\\" = \"+str(query.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of titles that start with ”The” = 48684\n",
      "Number of titles that start with ”The” and are not in \"en\" = 11553\n"
     ]
    }
   ],
   "source": [
    "f10(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f11(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # convert to double from string\n",
    "    df_normal = df_normal.withColumn('size', col('size').cast('double'))\n",
    "    df_normal = df_normal.withColumn('hits', col('hits').cast('double'))\n",
    "    \n",
    "    # Determine the percentage of pages that have only received a single page view in this one-hour of log data.\n",
    "    query = df_normal.filter(df_normal.hits == 1).count()\n",
    "    print(str(round(100*query/df_normal.count(),2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.38%\n"
     ]
    }
   ],
   "source": [
    "f11(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f12(rdd):\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    # Determine the number of unique terms appearing in the page titles. Note that in page\n",
    "    # titles, terms are delimited by \"_\" instead of a whitespace.\n",
    "    df = df_normal.select(F.split(F.col(\"title\"), \"_\")).rdd.map(lambda r : r[0])\n",
    "    print(df.flatMap(set).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3491232\n"
     ]
    }
   ],
   "source": [
    "f12(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f13(rdd):\n",
    "    # Determine the most frequently occurring page title term in this dataset.\n",
    "    input_data = rdd.map(lambda line: line.split(\" \"))\n",
    "    df_normal = input_data.toDF(['code','title','hits', 'size'])\n",
    "    \n",
    "    df = df_normal.select(F.split(F.col(\"title\"), \"_\")).rdd.map(lambda r : r[0])\n",
    "    df = df.flatMap(lambda doc: [(x, 1) for x in doc]).reduceByKey(add)\n",
    "    df = df.toDF(['word', 'count'])\n",
    "    x = df.select([max('count')]).collect()[0][0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215677\n"
     ]
    }
   ],
   "source": [
    "print(f13(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "sc.stop()\n",
    "conf = SparkConf().setAppName('appName').setMaster('spark://Jeans-MBP:7077')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# define rdd\n",
    "sqlContext = SQLContext(sc)\n",
    "rdd = sc.textFile(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small cluster configuration:\n",
    "\n",
    "SPARK_WORKER_CORES=2\n",
    "\n",
    "SPARK_WORKER_INSTANCES=2\n",
    "\n",
    "SPARK_WORKER_MEMORY=1g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3 takes 19.41314172744751 seconds\n",
      "Query 5 takes 44.53948783874512 seconds\n",
      "Query 6 takes 18.51176691055298 seconds\n",
      "Query 7 takes 16.548261880874634 seconds\n",
      "Query 13 takes 69.00603985786438 seconds\n"
     ]
    }
   ],
   "source": [
    "# Query 3\n",
    "start_time = time.time()\n",
    "f3(rdd)\n",
    "print(\"Query 3 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 5\n",
    "start_time = time.time()\n",
    "f5(rdd)\n",
    "print(\"Query 5 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 6\n",
    "start_time = time.time()\n",
    "f6(rdd)\n",
    "print(\"Query 6 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 7\n",
    "start_time = time.time()\n",
    "f7(rdd)\n",
    "print(\"Query 7 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 13\n",
    "start_time = time.time()\n",
    "f13(rdd)\n",
    "print(\"Query 13 takes %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Big cluster configuration:__\n",
    "\n",
    "SPARK_WORKER_CORES=8\n",
    "\n",
    "SPARK_WORKER_INSTANCES=8\n",
    "\n",
    "SPARK_WORKER_MEMORY=8g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3 takes 18.175821781158447 seconds\n",
      "Query 5 takes 36.74556493759155 seconds\n",
      "Query 6 takes 17.101213932037354 seconds\n",
      "Query 7 takes 15.644676923751831 seconds\n",
      "Query 13 takes 60.782407999038696 seconds\n"
     ]
    }
   ],
   "source": [
    "# Query 3\n",
    "start_time = time.time()\n",
    "f3(rdd)\n",
    "print(\"Query 3 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 5\n",
    "start_time = time.time()\n",
    "f5(rdd)\n",
    "print(\"Query 5 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 6\n",
    "start_time = time.time()\n",
    "f6(rdd)\n",
    "print(\"Query 6 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 7\n",
    "start_time = time.time()\n",
    "f7(rdd)\n",
    "print(\"Query 7 takes %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Query 13\n",
    "start_time = time.time()\n",
    "f13(rdd)\n",
    "print(\"Query 13 takes %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"SPARK_WORKER_CORES\" is the number of cores per worker. \"SPARK_WORKER_INSTANCES\" is the number of individual workers in the cluster. \"SPARK_WORKER_MEMORY\" is the amount of memory allocated to each cluster. \n",
    "\n",
    "The big cluster takes significantly more time to compute the queries 3, 5 and 13. This is because as one adds more and more nodes to the cluster, communication cost increases and therefore computing time increases. For queries 6 and 7 the computing time is about the same probably because the queries require less computation power and therefore the extra computation power of the big cluster is not exploited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
